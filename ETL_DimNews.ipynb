{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39e6021",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (StructType, StructField, StringType,\n",
    "                               IntegerType, BooleanType, TimestampType,\n",
    "                              ArrayType, MapType, DateType)\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder.appName(\"DimNews\") \\\n",
    "        .config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.24.0\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "#load news DF\n",
    "news_Df = spark \\\n",
    "        .read.option(\"recursiveFileLookup\", \"true\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .parquet(\"gs://stocks-pipeline/raw-data/sock_news\")\n",
    "\n",
    "\n",
    "news_Df = news_Df.withColumn(\"newsKey\", f.row_number().over(Window.orderBy('title')))\n",
    "news_Df=news_Df.dropDuplicates([\"id\"])\n",
    "\n",
    "news_Df = news_Df.drop(\"ticket\")\n",
    "news_Df = news_Df.withColumn('published_utc', f.split(news_Df['published_utc'], 'T').getItem(0))\n",
    "news_Df = news_Df.withColumnRenamed(\"published_utc\",\"PublishedUtc\") \\\n",
    "            .withColumnRenamed(\"amp_url\", \"AmpUrl\") \\\n",
    "            .withColumnRenamed(\"article_url\", \"ArticleUrl\") \\\n",
    "            .withColumnRenamed(\"image_url\", \"ImageUrl\") \\\n",
    "            .withColumnRenamed(\"id\", \"ArticleId\")\n",
    "            \n",
    "news_Df = news_Df.withColumn(\"PublishedUtc\",f.col(\"PublishedUtc\").cast(DateType()))\n",
    "news_Df = news_Df.fillna(value = 'Unknown', subset = [\"AmpUrl\"])\n",
    "\n",
    "persist_df = news_Df.persist()\n",
    "\n",
    "# persist_df = persist_df.drop(\"tickers\")\n",
    "persist_df = persist_df.drop(\"keywords\")\n",
    "\n",
    "print(persist_df.show())\n",
    "    \n",
    "    \n",
    "sd=persist_df.filter(\"newsKey = 19301\").select(\"tickers\").show()\n",
    "print(sd)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19b67571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "chunk_size = 50000\n",
    "i=0\n",
    "window = Window.orderBy('newsKey')\n",
    "persist_Df = persist_df.withColumn(\"row_idx\", row_number().over(window))\n",
    "\n",
    "\n",
    "# Loop through the dataframe in chunks\n",
    "while i < persist_Df.count():\n",
    "    \n",
    "    # Select the chunk of data\n",
    "    chunk=persist_Df.orderBy('newsKey').filter(persist_Df[\"row_idx\"] >= i).take(chunk_size)\n",
    "\n",
    "    # create a BigQuery client and dataset reference\n",
    "    client = bigquery.Client(project='noted-span-377814')\n",
    "    dataset_ref = client.dataset('Stocks_DW')\n",
    "\n",
    "    # create a BigQuery table and upload the data\n",
    "    table_ref = dataset_ref.table('DimNews')\n",
    "    chunk_df = spark.createDataFrame(chunk).drop(\"row_idx\")\n",
    "    job_config = bigquery.LoadJobConfig(write_disposition='WRITE_APPEND')\n",
    "    job = client.load_table_from_dataframe(chunk_df.toPandas(), table_ref, job_config=job_config)\n",
    "    i+=chunk_size\n",
    "    \n",
    "    print(job.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ca307c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fill bridge table\n",
    "\n",
    "bridge_Df_news = news_Df.select(\"newsKey\",\"tickers\").withColumn(\"tickers_exploded\", f.explode(\"tickers\")).drop(\"tickers\")\n",
    "\n",
    "# load tickers DF\n",
    "ticker_Df = spark.read.format(\"parquet\") \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .load(\"gs://stocks-pipeline/raw-data/ticker_details/*.parquet\")\n",
    "\n",
    "ticker_Df = ticker_Df.withColumn(\"tickerKey\", f.row_number().over(Window.orderBy('ticker')))\n",
    "bridge_Df_ticker = ticker_Df.select(\"tickerKey\", \"ticker\")\n",
    "\n",
    "# join bridge_Df_ticker and bridge_Df_news\n",
    "joined_Df = bridge_Df_ticker \\\n",
    "            .join(bridge_Df_news, bridge_Df_ticker.ticker == bridge_Df_news.tickers_exploded, \"inner\") \\\n",
    "            .select(\"tickerKey\", \"newsKey\")\n",
    "\n",
    "joined_Df = joined_Df.distinct()\n",
    "persist_Df = joined_Df.persist()\n",
    "\n",
    "print(persist_Df.show())\n",
    "\n",
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "chunk_size = 70000\n",
    "i=0\n",
    "window = Window.orderBy('newsKey')\n",
    "persist_Df = persist_Df.withColumn(\"row_idx\", row_number().over(window))\n",
    "\n",
    "\n",
    "# Loop through the dataframe in chunks\n",
    "while i < persist_Df.count():\n",
    "    # Select the chunk of data\n",
    "    chunk=persist_Df.orderBy('newsKey').filter(persist_Df[\"row_idx\"] >= i).take(chunk_size)\n",
    "\n",
    "    # create a BigQuery client and dataset reference\n",
    "    client = bigquery.Client(project='noted-span-377814')\n",
    "    dataset_ref = client.dataset('Stocks_DW')\n",
    "\n",
    "    # create a BigQuery table and upload the data\n",
    "    table_ref = dataset_ref.table('BrgNewsTicker')\n",
    "    chunk_df = spark.createDataFrame(chunk).select('tickerKey','newsKey')\n",
    "    job_config = bigquery.LoadJobConfig(write_disposition='WRITE_APPEND')\n",
    "    job = client.load_table_from_dataframe(chunk_df.toPandas(), table_ref, job_config=job_config)\n",
    "    i+=chunk_size\n",
    "    print(job.result())\n",
    "    \n",
    "    \n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
