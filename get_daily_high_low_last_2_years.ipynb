{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64a5cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (StructType, StructField, StringType, IntegerType,\n",
    "                            BooleanType, DoubleType,TimestampType,\n",
    "                            DateType, FloatType, LongType)\n",
    "from datetime import date, timedelta\n",
    "import json,time\n",
    "from pyspark.sql.functions import col, udf, from_unixtime, year, month, dayofmonth\n",
    "from datetime import date, timedelta\n",
    "\n",
    "spark = SparkSession.builder.appName(\"GetAllEnd\").getOrCreate()\n",
    "\n",
    "start_date = date(2023,1,21)\n",
    "end_date = date(2023,2,15)\n",
    "\n",
    "def get_business_days(start_date, end_date):\n",
    "    business_days = []\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        if current_date.weekday() < 5:\n",
    "            business_days.append(current_date)\n",
    "        current_date += timedelta(days=1)\n",
    "    return business_days\n",
    "\n",
    "dates = get_business_days(start_date, end_date)\n",
    "\n",
    "tickers_daily_schema = StructType([\n",
    "    StructField(\"T\", StringType(), nullable=True),\n",
    "    StructField(\"v\", StringType(), nullable=True),\n",
    "    StructField(\"vw\", StringType(), nullable=True),\n",
    "    StructField(\"o\", StringType(), nullable=True),\n",
    "    StructField(\"c\", StringType(), nullable=True),\n",
    "    StructField(\"h\", StringType(), nullable=True),\n",
    "    StructField(\"l\", StringType(), nullable=True),\n",
    "    StructField(\"t\", StringType(), nullable=True),\n",
    "    StructField(\"n\", StringType(), nullable=True)\n",
    "])\n",
    "\n",
    "def api_call(date):\n",
    "    url = f\"https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/{date}?adjusted=true&apiKey=VOYRq_vnd9soI0AhCO8FQpK4auZ6ppc3\"\n",
    "    response = requests.get(url)\n",
    "    try:\n",
    "        var = response.json()['results']\n",
    "        return var\n",
    "    except:\n",
    "        return 0 \n",
    "\n",
    "start_time=60.0\n",
    "end_time=1.0\n",
    "INTERVAL_SECONDS = 60\n",
    "new_column_names = ['ticker', 'volume', 'volume_weighted', 'opening_price', 'closing_price', 'highest_price', 'lowest_price', 'unix_time', 'no_of_transactions']\n",
    "\n",
    "while len(dates)>0:\n",
    "    \n",
    "    if len(dates)>4:\n",
    "        elapsed_time = start_time - end_time\n",
    "        remaining_time = max(0, INTERVAL_SECONDS - elapsed_time)\n",
    "        time.sleep(remaining_time)\n",
    "        start_time=time.time()\n",
    "        for date in dates[:5]:\n",
    "            returned_json = api_call(date)\n",
    "            if returned_json!=0:\n",
    "                daily_high_low_df = spark.createDataFrame(returned_json, tickers_daily_schema)\n",
    "                daily_high_low_df=daily_high_low_df.toDF(*new_column_names)\n",
    "                daily_high_low_df= daily_high_low_df.withColumn(\"datetime\", from_unixtime(daily_high_low_df[\"unix_time\"]/1000))\n",
    "                daily_high_low_df = daily_high_low_df.withColumn(\"year\", year(\"datetime\").cast(IntegerType()))\n",
    "                daily_high_low_df = daily_high_low_df.withColumn(\"month\", month(\"datetime\").cast(IntegerType()))\n",
    "                daily_high_low_df = daily_high_low_df.withColumn(\"day\", dayofmonth(\"datetime\").cast(IntegerType()))\n",
    "                daily_high_low_df = daily_high_low_df.repartition(8)\n",
    "                daily_high_low_df.repartition(\"year\", \"month\", \"day\") \\\n",
    "                .sortWithinPartitions(\"ticker\") \\\n",
    "                .write \\\n",
    "                .mode(\"append\") \\\n",
    "                .partitionBy(\"year\", \"month\", \"day\") \\\n",
    "                .format(\"parquet\") \\\n",
    "                .option(\"compression\", \"snappy\") \\\n",
    "                .save(\"gs://stocks-pipeline/raw-data/daily_high_low\")\n",
    "#             print(daily_high_low_df.show(1))\n",
    "        dates[:5] = []\n",
    "        end_time=time.time()\n",
    "    else:\n",
    "        for date in dates:\n",
    "            returned_json = api_call(date)\n",
    "            if returned_json!=0:\n",
    "                daily_high_low_df = spark.createDataFrame(returned_json, tickers_daily_schema)\n",
    "                daily_high_low_df=daily_high_low_df.toDF(*new_column_names)\n",
    "                daily_high_low_df= daily_high_low_df.withColumn(\"datetime\", from_unixtime(daily_high_low_df[\"unix_time\"]/1000))\n",
    "                daily_high_low_df = daily_high_low_df.withColumn(\"year\", year(\"datetime\").cast(IntegerType()))\n",
    "                daily_high_low_df = daily_high_low_df.withColumn(\"month\", month(\"datetime\").cast(IntegerType()))\n",
    "                daily_high_low_df = daily_high_low_df.withColumn(\"day\", dayofmonth(\"datetime\").cast(IntegerType()))\n",
    "                daily_high_low_df = daily_high_low_df.repartition(8)\n",
    "                daily_high_low_df.repartition(\"year\", \"month\", \"day\") \\\n",
    "                .sortWithinPartitions(\"ticker\") \\\n",
    "                .write \\\n",
    "                .mode(\"append\") \\\n",
    "                .partitionBy(\"year\", \"month\", \"day\") \\\n",
    "                .format(\"parquet\") \\\n",
    "                .option(\"compression\", \"snappy\") \\\n",
    "                .save(\"gs://stocks-pipeline/raw-data/daily_high_low\")\n",
    "            \n",
    "        dates =[]\n",
    "\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
